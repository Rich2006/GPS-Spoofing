# -*- coding: utf-8 -*-
"""ALL. LSTM Autoencoder h480 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v2LQ5tkkHiIkS-GOMdxP_pTmSF6B0Zmt
"""

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/My Drive/Colab Notebooks/HEX New folder'

# Commented out IPython magic to ensure Python compatibility.
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
# tf.logging.set_verbosity(tf.logging.ERROR)

from tensorflow import keras
import seaborn as sns
sns.set(color_codes=True)
from scipy import stats
import matplotlib.pyplot as plt

from pylab import rcParams
from matplotlib import rc
from pandas.plotting import register_matplotlib_converters

import statsmodels.api as sm
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.externals import joblib

from keras import regularizers
from keras.layers import RepeatVector
from keras.layers import TimeDistributed


tf.keras.layers.Layer
# import pandas as pd
# from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
# from keras.models import Model
# import seaborn as sns



import tensorflow as tf
from tensorflow.keras.models import Sequential
# from keras.layers import Dense
# from keras.layers import LSTM
# from keras.layers import Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
from tensorflow import keras
from tensorflow.keras.layers import Convolution2D, MaxPool2D, Flatten, Dense, LSTM,Dropout


from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

register_matplotlib_converters()
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 22, 10

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

import pandas.util.testing as tm

def ReshapeY(Y_train,n):
    Y = list()
    for x in Y_train:
        Y.append(find_1(x, n))

    Y = np.array(Y)
    return Y
    print(Y.shape)
    
    
# look for 1 ( spoof) in each   
def find_1(x, n):
    if 1 in x:
        res = 1
    else: 
        res = 0
    return res    




    
def LOAD_data(path ):
    filenames = glob.glob(path + "/*.csv")

    dfs = []
    for filename in filenames:
        df=pd.read_csv(filename)
        if 'le0.csv'== filename[-7:]:
            df['attack'] = 0
            df = df[190:] 

        else:
            df['attack'] = 1
        dfa = df['attack']
        df = df[14:]
        df = df.iloc[:-180]
        df = df.select_dtypes(exclude=['object','bool'])         #remove nan
        df = df.loc[:, (df != 0).any(axis=0)]                    #remove zeros
        df = df.drop(df.std()[(df.std() == 0)].index, axis=1)   #remove equals
        df=((df-df.min())/(df.max()-df.min()))*1

        df['attack'] = dfa
        dfs.append(df)


    # Concatenate all data into one DataFrame
    df = pd.concat(dfs, ignore_index=True)
        #df.head()
        
    
    # Concatenate all data into one DataFrame
    df = pd.concat(dfs, ignore_index=True)
    #df.head()

    df = df.select_dtypes(exclude=['object','bool'])         #remove nan
    df = df.loc[:, (df != 0).any(axis=0)]                    #remove zeros
    df = df.drop(df.std()[(df.std() == 0)].index, axis=1)   #remove equals

    sf = df[['roll', 'pitch', 'heading', 'rollRate', 'pitchRate', 'yawRate',
       'groundSpeed',  'altitudeRelative', 
       'throttlePct', 'estimatorStatus.horizPosRatio',
       'estimatorStatus.vertPosRatio',
       'estimatorStatus.horizPosAccuracy','gps.courseOverGround']]
    scaled_data = scale(sf)
    

    pca = PCA(n_components = 9)
    pca.fit(scaled_data)
    pca_data = pca.transform(scaled_data)

    pca_data = pd.DataFrame(pca_data)

    df_sf = pd.concat([pca_data, df[['attack']]], axis=1)

    sf_t =df_sf

    data_dim = sf_t.shape[1] -1
    timesteps = 60
    num_classes = 2


    # X = sf_t.drop(['attack'], axis =1).values
    # Y = sf_t[['attack']].values



    # ll = sf_t.shape[0] // timesteps
    # ll

    # x = np.array(X[0: (timesteps*ll)])
    # y = np.array(Y[0: (timesteps*ll)])
    # x.shape

    # X_t = np.reshape(x,(-1,timesteps,data_dim))
    # Y_t = np.reshape(y,(-1,timesteps,1))


    # Y_t = ReshapeY(Y_t,timesteps )
    # print(X_t.shape)
    # print(Y_t.shape)

    # lb_make = LabelEncoder()
    # Y_t = lb_make.fit_transform(Y_t)
    # Y_t = tf.keras.utils.to_categorical(Y_t)
    # X_t = X_t.astype("float32")
    # Y_t = Y_t.astype("float32")
    # X_t /= 255
    
    # return (X_t,Y_t)
    return (sf_t)


def put_together(combined_array, asd):
    combined_array = np.concatenate((combined_array, asd), axis=0)
    #combined_array = np.delete(combined_array, 0, axis=0)
    return combined_array


def Delete_first(combined_array):
    combined_array = np.delete(combined_array, 0, axis=0)
    return combined_array

import os
 
paths = []    
# rootdir = r'C:\Users\lenovo\OneDrive - aggies.ncat.edu\Desktop\new correct files\HEX New folder'
for file in os.listdir(data_dir):
    d = os.path.join(data_dir, file)
    if os.path.isdir(d):
        paths.append(d)

paths

# from sklearn.preprocessing import scale

# i = 0
# for path in paths:
#     (Xa,Ya) = LOAD_data(path)
#     if  (i == 0):
#         X_ = Xa  
#         Y_ = Ya
#         i = i + 1 
#     else:
#         X_ = np.concatenate((X_, Xa), axis=0)
#         Y_ = np.concatenate((Y_, Ya), axis=0)

from sklearn.preprocessing import scale

i = 0
for path in paths:
    (dd) = LOAD_data(path)
    if  (i == 0):
        dat = dd  
        i = i + 1
    else:
        dat = np.concatenate((dat, dd), axis=0)

pca_data = pd.DataFrame(dat)
pca_data

train = pca_data.loc[pca_data[9] == 1]
# test = sf_t.loc[sf_t['attack'] == "True"]
# test = test[57186:]
# test = test

df = train.drop(train.std()[(train.std() == 0)].index, axis=1)   #remove equals
df

# train = sf.loc[sf['attack'] == 1]
# test = sf.loc[sf['attack'] == -1]
# test = test[0:7186]

train = df

test1 = pca_data.loc[pca_data[9] == 0]
test = test1.drop(test1.std()[(test1.std() == 0)].index, axis=1)   #remove equals

# normalize the data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(train)
X_test = scaler.transform(test)
scaler_filename = "scaler_data"
joblib.dump(scaler, scaler_filename)

# data_dim = X_train.shape[1] 
# timesteps = 10
# num_classes = 2

# # workthe int
# ll = sf.shape[0] // timesteps
# ll

# x = np.array(X_train[0: (timesteps*ll)])
# # y = np.array(Y[0: (timesteps*ll)])

# X_train =np.reshape(x,(-1,timesteps,data_dim))
# print("Training data shape:", X_train.shape)

# X_test = np.array(X_test[0: (timesteps*ll)])
# X_test =np.reshape(X_test,(-1,timesteps,data_dim))
# print("Test data shape:", X_test.shape)

# reshape inputs for LSTM [samples, timesteps, features]
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
print("Training data shape:", X_train.shape)
X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])
print("Test data shape:", X_test.shape)

# define the autoencoder network model
def autoencoder_model(X):
    inputs = Input(shape=(X.shape[1], X.shape[2]))
    L1 = LSTM(16, activation='relu', return_sequences=True, 
              kernel_regularizer=regularizers.l2(0.00))(inputs)
    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)
    L3 = RepeatVector(X.shape[1])(L2)
    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)
    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)
    output =  tf.keras.layers.TimeDistributed(Dense(X.shape[2]))(L5)
    # output = tf.keras.layers.TimeDistributed(L5)(X.shape[2])   
    model = Model(inputs=inputs, outputs=output)
    return model

from keras import regularizers
# create the autoencoder model
model = autoencoder_model(X_train)
model.compile(optimizer='adam', loss='mae')
model.summary()

# fit the model to the data
nb_epochs = 100
batch_size = 10
history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,
                    validation_split=0.2).history

# plot the training losses
fig, ax = plt.subplots(figsize=(14, 6), dpi=80)
ax.plot(history['loss'], 'b', label='Train', linewidth=2)
ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)
ax.set_title('Model loss', fontsize=16)
ax.set_ylabel('Loss (mae)')
ax.set_xlabel('Epoch')
ax.legend(loc='upper right')
plt.show()

# plot the loss distribution of the training set
X_pred = model.predict(X_train)
X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])
X_pred = pd.DataFrame(X_pred, columns=train.columns)
X_pred.index = train.index

scored = pd.DataFrame(index=train.index)
Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])
scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis = 1)
plt.figure(figsize=(16,9), dpi=80)
plt.title('Loss Distribution', fontsize=16)
sns.distplot(scored['Loss_mae'], bins = 20, kde= True, color = 'blue');
plt.xlim([0.0,.5])

# calculate the loss on the test set
X_pred = model.predict(X_test)
X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])
X_pred = pd.DataFrame(X_pred, columns=test.columns)
X_pred.index = test.index

scored = pd.DataFrame(index=test.index)
Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])
scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)
scored['Threshold'] = 0.275
scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']
scored.head()

# calculate the same metrics for the training set 
# and merge all data in a single dataframe for plotting
X_pred_train = model.predict(X_train)
X_pred_train = X_pred_train.reshape(X_pred_train.shape[0], X_pred_train.shape[2])
X_pred_train = pd.DataFrame(X_pred_train, columns=train.columns)
X_pred_train.index = train.index

scored_train = pd.DataFrame(index=train.index)
scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-Xtrain), axis = 1)
scored_train['Threshold'] = 0.275
scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']
scored = pd.concat([scored_train, scored])

# plot bearing failure time plot
scored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])